{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "task_1_matt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvqFYlBnp6SB"
      },
      "source": [
        "# Non-Pretrained Methods!\n",
        "\n",
        "All code for our non-pretrained methods is contained in this notebook.\n",
        "\n",
        "The bulk of the work for each method is encapsulated in a dedicated class, with some methods having testing functions which can be used to repeat our experiments. This notebook also contains some of our preliminary results, but these are not necessarily final, please refer to our report for our final results for each method.\n",
        "\n",
        "Prior to the definition of our methods we load the data and import the necessary libraries. All validation/test errors were calculated using the model_performance function, as provided in the original skeleton.\n",
        "\n",
        "Here is a brief outline of our code/how to repeat experiments:\n",
        "\n",
        " 1. Regression on TF-IDF features\n",
        "    - Method class: TfidfRegressor\n",
        "    - Experiment function: test_regressor\n",
        "    - How to repeat experiments:\n",
        "      - To set min_df, you must pass it as a hardcoded parameter to\n",
        "        TfidfRegressor. This sets the minimum number of headlines a word must\n",
        "        feature in to be included in the model.\n",
        "      - For all other parameters, call test_regressor with the desired\n",
        "        experiment parameters (examples in code)\n",
        "      - Performance on Dev/Validation set and test set will be printed.\n",
        " 2. Predicting funiness from Perplexity\n",
        "    - WARNING: Running the perplexity experiments takes a long time, due to\n",
        "      first training a language model, and then computing the perplexity of\n",
        "      each item in the training dataset (see method classes for details).\n",
        "    - **Perplexity of whole headline**\n",
        "      - Method class: SentenceLMFunnyEstimator\n",
        "      - Experiment function: test_sentence_perplexity_model\n",
        "      - How to repeat experiments:\n",
        "        - Code for all experiments reported are in cells following\n",
        "          test_sentence_perplexity_model\n",
        "        - Call test_sentence_perplexity_model with appropriate experiment\n",
        "          parameters\n",
        "        - MAKE SURE YOU USE THE CORRECT TYPE OF DATASETS (see comments above\n",
        "          test_sentence_perplexity_model\n",
        "        - Performance on Dev/Validation set will be printed.\n",
        "    - **Perplexity of n-grams around edit**\n",
        "      - Method class: EditContextLMFunnyEstimator\n",
        "      - Test function: None :'(\n",
        "      - How to repeat experiments:\n",
        "        - Code for running all the experiments is in the code cells below\n",
        "          EditContextLMFunnyEstimator, with comments relating each experiment\n",
        "          to the entry in Table 3 of the report.\n",
        " 3. Averaging across part-of-speech tags\n",
        "    - Method class: POSTagFunninessPredictor\n",
        "    - Experiment function: test_pos_predictor\n",
        "    - How to repeat experiments:\n",
        "      - Call test_pos_predictor with experiment parameters.\n",
        "      - Performance on Dev/Validation set and test set will be printed.\n",
        "      - Examples of running experiment are given below test_pos_predictor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgLa8arIp6SD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff21221-0f66-41e2-eca5-e2a8826ebcd9"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip\n",
        "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "# !unzip glove.twitter.27B.zip\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "! pip install torch\n",
        "! pip install nltk==3.5\n",
        "! pip install truecase\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434676 sha256=002bf10f92a21dfce3543fd199f4f75bc2f2f69fba675c4c71f0a46d2fdf464c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n",
            "Collecting truecase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/52/0824cdadfe0b924f1f10b3a4042b2e15ae2477ed8acc032418a449a62936/truecase-0.0.12-py3-none-any.whl (28.4MB)\n",
            "\u001b[K     |████████████████████████████████| 28.4MB 149kB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from truecase) (3.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (7.1.2)\n",
            "Installing collected packages: truecase\n",
            "Successfully installed truecase-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdNtrSvMp6SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3787b5-dd2a-411e-86ce-479439616fe6"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "\n",
        "import multiprocessing\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
        "\n",
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
        "from nltk.lm import MLE, Laplace\n",
        "from nltk.util import pad_sequence, everygrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.chunk.util import tree2conlltags\n",
        "\n",
        "from truecase import get_true_case\n",
        "\n",
        "nltk.download('popular')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzzCcfw-p6SF"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46OQnc_Jp6SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7781e754-6c70-4353-f2d9-82aece03eec0"
      },
      "source": [
        "# Load data\n",
        "!wget https://cs.rochester.edu/u/nhossain/semeval-2020-task-7-dataset.zip\n",
        "!unzip semeval-2020-task-7-dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-27 20:44:03--  https://cs.rochester.edu/u/nhossain/semeval-2020-task-7-dataset.zip\n",
            "Resolving cs.rochester.edu (cs.rochester.edu)... 192.5.53.208\n",
            "Connecting to cs.rochester.edu (cs.rochester.edu)|192.5.53.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1621456 (1.5M) [application/zip]\n",
            "Saving to: ‘semeval-2020-task-7-dataset.zip’\n",
            "\n",
            "semeval-2020-task-7 100%[===================>]   1.55M  8.12MB/s    in 0.2s    \n",
            "\n",
            "2021-02-27 20:44:04 (8.12 MB/s) - ‘semeval-2020-task-7-dataset.zip’ saved [1621456/1621456]\n",
            "\n",
            "Archive:  semeval-2020-task-7-dataset.zip\n",
            "   creating: semeval-2020-task-7-dataset/\n",
            "  inflating: semeval-2020-task-7-dataset/.DS_Store  \n",
            "   creating: semeval-2020-task-7-dataset/subtask-1/\n",
            "  inflating: semeval-2020-task-7-dataset/subtask-1/train_funlines.csv  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-1/.DS_Store  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-1/test.csv  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-1/dev.csv  \n",
            " extracting: semeval-2020-task-7-dataset/subtask-1/baseline.zip  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-1/train.csv  \n",
            "  inflating: semeval-2020-task-7-dataset/README.txt  \n",
            "   creating: semeval-2020-task-7-dataset/subtask-2/\n",
            "  inflating: semeval-2020-task-7-dataset/subtask-2/train_funlines.csv  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-2/.DS_Store  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-2/test.csv  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-2/dev.csv  \n",
            " extracting: semeval-2020-task-7-dataset/subtask-2/baseline.zip  \n",
            "  inflating: semeval-2020-task-7-dataset/subtask-2/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znb4x_qVp6SH"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.5f} | RMSE: {rmse:.5f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bXriZyAx6YG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5830ae59-ae78-44aa-ad60-71616191f4b3"
      },
      "source": [
        "\n",
        "train_df = pd.read_csv('/content/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "train_fl_df = pd.read_csv('/content/semeval-2020-task-7-dataset/subtask-1/train_funlines.csv')\n",
        "val_df = pd.read_csv('/content/semeval-2020-task-7-dataset/subtask-1/dev.csv')\n",
        "test_df = pd.read_csv('/content/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "\n",
        "def extract_data(df):\n",
        "    raw_data = df['original']\n",
        "    edit_data = df['edit']\n",
        "    original_data = pd.Series([re.sub('<|\\/>', '', s) for s in raw_data])\n",
        "    edited_data = pd.Series([re.sub('<.*\\/>', e, s) for s, e in zip(raw_data, edit_data)])\n",
        "    grade_data = df['meanGrade']\n",
        "    return raw_data, edit_data, original_data, edited_data, grade_data\n",
        "\n",
        "train_raw, train_edit, train_original, train_edited, train_grades = extract_data(train_df)\n",
        "train_fl_raw, train_fl_edit, train_fl_original, train_fl_edited, train_fl_grades = extract_data(train_fl_df)\n",
        "val_raw, val_edit, val_original, val_edited, val_grades = extract_data(val_df)\n",
        "test_raw, test_edit, test_original, test_edited, test_grades = extract_data(test_df)\n",
        "\n",
        "\n",
        "print(train_raw[0])\n",
        "print(train_original[0])\n",
        "print(train_edited[0])\n",
        "\n",
        "# Combine all that data\n",
        "combined_train_raw = train_raw.append(train_fl_raw, ignore_index=True)\n",
        "combined_train_edit = train_edit.append(train_fl_edit, ignore_index=True)\n",
        "combined_train_original = train_original.append(train_fl_original, ignore_index=True)\n",
        "combined_train_edited = train_edited.append(train_fl_edited, ignore_index=True)\n",
        "combined_train_grades = train_grades.append(train_fl_grades, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "France is ‘ hunting down its citizens who joined <Isis/> ’ without trial in Iraq\n",
            "France is ‘ hunting down its citizens who joined Isis ’ without trial in Iraq\n",
            "France is ‘ hunting down its citizens who joined twins ’ without trial in Iraq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wz1k7V9p6SK"
      },
      "source": [
        "# Approach 1: TF-IDF Regression\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX1wi_i4p6SL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4047fbd-e05b-4396-e7f5-255f00866a94"
      },
      "source": [
        "\n",
        "class TfidfRegressor:\n",
        "\n",
        "    def __init__(self, train_corpus, train_values, rm=Lasso(alpha=0.0001), sw='english'):\n",
        "\n",
        "        self.tfidfVectorizer = TfidfVectorizer(stop_words=sw)\n",
        "\n",
        "        train_corpus_counts = self.tfidfVectorizer.fit_transform(train_corpus)\n",
        "        self.regression_model = rm.fit(train_corpus_counts, train_values)\n",
        "\n",
        "    def predict(self, sample_corpus):\n",
        "\n",
        "        sample_counts = self.tfidfVectorizer.transform(sample_corpus)\n",
        "        return self.regression_model.predict(sample_counts)\n",
        "\n",
        "\n",
        "# train_data is the dataset of edited headlines to train on\n",
        "# train_labels is the scores for those headlines\n",
        "# rm is the regression model to use (e.g. Lasso(0.0006) )\n",
        "# sw is type of stopwords to remove ('english' or None)\n",
        "# examples of how to run are below function definition\n",
        "def test_regressor(train_data, train_labels, rm, sw):\n",
        "    model = TfidfRegressor(train_data, train_labels, rm, sw)\n",
        "\n",
        "    print(\"Number of TF-IDF features\")\n",
        "    print(len(model.tfidfVectorizer.get_feature_names()))\n",
        "\n",
        "    print(\"Is this just predicting the mean score?\")\n",
        "    print(np.allclose(np.mean(train_labels), model.regression_model.intercept_))\n",
        "\n",
        "    # Evaluate performance\n",
        "    train_tfidf_preds = model.predict(train_data)\n",
        "    print(\"\\nTrain performance:\")\n",
        "    model_performance(train_tfidf_preds, train_labels, True)\n",
        "\n",
        "\n",
        "    val_tfidf_preds = model.predict(val_edited)\n",
        "    print(\"\\nDev performance:\")\n",
        "    model_performance(val_tfidf_preds, val_grades, True)\n",
        "\n",
        "    test_tfidf_preds = model.predict(test_edited)\n",
        "    print(\"\\nTest performance\")\n",
        "    model_performance(test_tfidf_preds, test_grades, True)\n",
        "    \n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "#test_regressor(combined_train_edited, combined_train_grades, LinearRegression(), None)\n",
        "\n",
        "test_regressor(combined_train_edited, combined_train_grades, Lasso(0.0002), 'english')\n",
        "test_regressor(combined_train_edited, combined_train_grades, Lasso(0.00006), 'english')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of TF-IDF features\n",
            "16283\n",
            "Is this just predicting the mean score?\n",
            "False\n",
            "\n",
            "Train performance:\n",
            "| MSE: 0.33 | RMSE: 0.58 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.35 | RMSE: 0.59 |\n",
            "\n",
            "Test performance\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n",
            "\n",
            "\n",
            "Number of TF-IDF features\n",
            "16283\n",
            "Is this just predicting the mean score?\n",
            "False\n",
            "\n",
            "Train performance:\n",
            "| MSE: 0.28 | RMSE: 0.53 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.35 | RMSE: 0.59 |\n",
            "\n",
            "Test performance\n",
            "| MSE: 0.33 | RMSE: 0.58 |\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBV8WHAXxqY1"
      },
      "source": [
        "## TF-IDF Regression Results\n",
        "\n",
        "## Edit Word Model (inc stop words):\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.14 | RMSE: 0.37 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.36 | RMSE: 0.60 |\n",
        "\n",
        "Test performance:\n",
        "| MSE: 0.37 | RMSE: 0.61 |\n",
        "\n",
        "## Editted Sentence Model:\n",
        "\n",
        "**Base**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.02 | RMSE: 0.14 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 22.15 | RMSE: 4.71 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 19.83 | RMSE: 4.45 |\n",
        "\n",
        "**No stop words**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.02 | RMSE: 0.16 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 8.95 | RMSE: 2.99 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 8.51 | RMSE: 2.92 |\n",
        "\n",
        "**No stop words, min_df=2**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.08 | RMSE: 0.28 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 3.23 | RMSE: 1.80 |\n",
        "\n",
        "**No stop words, min_df=3**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.11 | RMSE: 0.33 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 2.25 | RMSE: 1.50 |\n",
        "\n",
        "**No stop words, min_df=5**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.18 | RMSE: 0.42 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.61 | RMSE: 0.78 |\n",
        "\n",
        "**No stop words, min_df=10**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.25 | RMSE: 0.50 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.40 | RMSE: 0.63 |\n",
        "\n",
        "**No stop words, min_df=20**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.29 | RMSE: 0.54 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.35 | RMSE: 0.59 |\n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.5**\n",
        "\n",
        "(just predicts mean)\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.34 | RMSE: 0.58 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.33 | RMSE: 0.58 |\n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.0001**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.27 | RMSE: 0.52 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.33 | RMSE: 0.57 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 0.32 | RMSE: 0.56 |\n",
        "\n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.00005**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.20 | RMSE: 0.45 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.34 | RMSE: 0.59 |\n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.00001**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.07 | RMSE: 0.27 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.50 | RMSE: 0.70 |\n",
        "\n",
        "\n",
        "\n",
        "## Editted Sentence Model, trained on normal + FunLines\n",
        "\n",
        "**Baseline**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.04 | RMSE: 0.21 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 2.22 | RMSE: 1.49 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 2.25 | RMSE: 1.50 |\n",
        "\n",
        "**No stop words**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.05 | RMSE: 0.22 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 1.95 | RMSE: 1.40 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 1.92 | RMSE: 1.38 \n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.0001**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.31 | RMSE: 0.56 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.35 | RMSE: 0.59 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 0.33 | RMSE: 0.58 |\n",
        "\n",
        "**No stop words, Lasso Regression, alpha=0.00005**\n",
        "\n",
        "Train performance:\n",
        "| MSE: 0.26 | RMSE: 0.51 |\n",
        "\n",
        "Dev performance:\n",
        "| MSE: 0.35 | RMSE: 0.59 |\n",
        "\n",
        "Test performance\n",
        "| MSE: 0.33 | RMSE: 0.58 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXi21q7pEBKs"
      },
      "source": [
        "# Approach 2: Relating Perplexity to Funniness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYVg64nfS1d4"
      },
      "source": [
        "## Language models/perplexity\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    return [t for t in re.split('\\s+', s) if t is not '']\n",
        "\n",
        "# Idea: train LM on normal data, then find correlation between perplexity\n",
        "# of LM on edited data and funniness\n",
        "class SentenceLMFunnyEstimator():\n",
        "\n",
        "    def __init__(self, order, normal_data, funny_data, funny_labels):\n",
        "\n",
        "        self.order = order\n",
        "\n",
        "        train_normal_corp = [tokenize(s) for s in normal_data]\n",
        "\n",
        "        train_d, train_v = padded_everygram_pipeline(order, train_normal_corp)\n",
        "\n",
        "        # MLE break due to missing words?\n",
        "        self.lm = Laplace(order)\n",
        "        self.lm.fit(train_d, train_v)\n",
        "\n",
        "        train_funny_corp = [tokenize(s) for s in funny_data]\n",
        "        print(len(train_funny_corp))\n",
        "\n",
        "        perps = self.get_perplexities(train_funny_corp)\n",
        "        perps = np.expand_dims(np.array(perps), axis=1)\n",
        "\n",
        "        self.regressor = LinearRegression().fit(perps, funny_labels)\n",
        "\n",
        "        train_preds = self.regressor.predict(perps)\n",
        "\n",
        "        print(\"Train Performance:\")\n",
        "        print()\n",
        "        model_performance(train_preds, funny_labels, True)\n",
        "        print()\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        # Assume tokenised sentence\n",
        "        perps = self.get_perplexities(sentences)\n",
        "\n",
        "        np_perps = np.expand_dims(np.array(perps), axis=1)\n",
        "\n",
        "        return self.regressor.predict(np_perps)\n",
        "\n",
        "    def _get_perplexity(self, sentence):\n",
        "        return self.lm.perplexity(padded_everygrams(self.order, sentence))\n",
        "\n",
        "    def get_perplexities(self, sentences):\n",
        "        with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "            return pool.map(self._get_perplexity, sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkmYJK1QKQdv"
      },
      "source": [
        "val_toks =  [[t for t in s.split(' ') if t != ''] for s in val_edited]\r\n",
        "\r\n",
        "# ngram - N of n-gram language model to use\r\n",
        "# data_orig - uneditted headlines for training set to use\r\n",
        "#             (all are named <something>_original )\r\n",
        "# data_edited - edited headlines for training set to use\r\n",
        "#               (all are named <something>_edited )\r\n",
        "# data_grades - scores of data_edited headlines\r\n",
        "#               (all are named <something>_grades)\r\n",
        "def test_sentence_perplexity_model(ngram, data_orig, data_edited, data_grades):\r\n",
        "\r\n",
        "    lfme = SentenceLMFunnyEstimator(ngram, data_orig, data_edited, data_grades)\r\n",
        "    preds = lfme.predict(val_toks)\r\n",
        "    \r\n",
        "    print(\"Validation Performance\")\r\n",
        "    print()\r\n",
        "    sse, mse = model_performance(preds, val_grades, True)\r\n",
        "    print()\r\n",
        "    print(\"Regressor intercept: \", lfme.regressor.intercept_)\r\n",
        "    print()\r\n",
        "    print(\"Regressor perplexity coefficient: \", lfme.regressor.coef_)\r\n",
        "    print()\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQAn3PlGa7to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2ff9dd-d0a1-449b-de37-e0dfff103235"
      },
      "source": [
        "\n",
        "print('Base dataset results')\n",
        "print('3 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(3, train_original, train_edited, train_grades)\n",
        "print('2 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(2, train_original, train_edited, train_grades)\n",
        "print('1 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(1, train_original, train_edited, train_grades)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base dataset results\n",
            "3 gram lfme\n",
            "\n",
            "9652\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.33839 | RMSE: 0.58172 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.35156 | RMSE: 0.59293 |\n",
            "\n",
            "Regressor intercept:  1.094308667001804\n",
            "\n",
            "Regressor perplexity coefficient:  [-0.00014175]\n",
            "\n",
            "\n",
            "2 gram lfme\n",
            "\n",
            "9652\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.33787 | RMSE: 0.58127 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.33970 | RMSE: 0.58284 |\n",
            "\n",
            "Regressor intercept:  1.0641348931340255\n",
            "\n",
            "Regressor perplexity coefficient:  [-7.66573025e-05]\n",
            "\n",
            "\n",
            "1 gram lfme\n",
            "\n",
            "9652\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.33894 | RMSE: 0.58218 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.33187 | RMSE: 0.57608 |\n",
            "\n",
            "Regressor intercept:  0.9824355881975723\n",
            "\n",
            "Regressor perplexity coefficient:  [-1.48554245e-05]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRFmqBjTWfLw",
        "outputId": "76b3780b-97b4-4058-aa68-0f65814e57e6"
      },
      "source": [
        "\n",
        "print('Combined dataset results')\n",
        "print('3 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(3, combined_train_original, combined_train_edited, combined_train_grades)\n",
        "print('2 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(2, combined_train_original, combined_train_edited, combined_train_grades)\n",
        "print('1 gram lfme')\n",
        "print()\n",
        "test_sentence_perplexity_model(1, combined_train_original, combined_train_edited, combined_train_grades)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Combined dataset results\n",
            "3 gram lfme\n",
            "\n",
            "17900\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35496 | RMSE: 0.59578 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.37317 | RMSE: 0.61087 |\n",
            "\n",
            "Regressor intercept:  1.0417056013230948\n",
            "\n",
            "Regressor perplexity coefficient:  [2.71052366e-05]\n",
            "\n",
            "\n",
            "2 gram lfme\n",
            "\n",
            "17900\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35518 | RMSE: 0.59597 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.35436 | RMSE: 0.59528 |\n",
            "\n",
            "Regressor intercept:  1.1093102757298205\n",
            "\n",
            "Regressor perplexity coefficient:  [-7.37558235e-06]\n",
            "\n",
            "\n",
            "1 gram lfme\n",
            "\n",
            "17900\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35438 | RMSE: 0.59530 |\n",
            "\n",
            "Validation Performance\n",
            "\n",
            "| MSE: 0.35426 | RMSE: 0.59520 |\n",
            "\n",
            "Regressor intercept:  1.121725776362903\n",
            "\n",
            "Regressor perplexity coefficient:  [-7.73702134e-06]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpiRN4kA4qDy"
      },
      "source": [
        "# Idea: Same as above, but now just focus on fitting perplexity of context\n",
        "# around edit\n",
        "\n",
        "\n",
        "class EditContextLMFunnyEstimator():\n",
        "\n",
        "    def __init__(self, order, normal_data, unprocessed_data, edit_words, funny_labels):\n",
        "\n",
        "        self.order = order\n",
        "\n",
        "        train_normal_corp = [tokenize(s) for s in normal_data]\n",
        "\n",
        "        train_d, train_v = padded_everygram_pipeline(order, train_normal_corp)\n",
        "\n",
        "        # MLE break due to missing words?\n",
        "        self.lm = Laplace(order)\n",
        "        self.lm.fit(train_d, train_v)\n",
        "\n",
        "        e_context_everygram = self.extract_edit_context(unprocessed_data, edit_words)\n",
        "\n",
        "        perps = self.get_perplexities(e_context_everygram)\n",
        "        perps = np.expand_dims(np.array(perps), axis=1)\n",
        "\n",
        "        self.regressor = LinearRegression().fit(perps, funny_labels)\n",
        "\n",
        "        train_preds = self.regressor.predict(perps)\n",
        "\n",
        "        print(\"Train Performance:\")\n",
        "        print()\n",
        "        model_performance(train_preds, funny_labels, True)\n",
        "        print()\n",
        "\n",
        "    def extract_edit_context(self, unprocessed_data, editwords):\n",
        "\n",
        "        edits_with_context = []\n",
        "\n",
        "        for s, e in zip(unprocessed_data, editwords):\n",
        "            first_half, second_half = re.split('<.*\\/>', s)\n",
        "            \n",
        "            fh_toks = tokenize(first_half)\n",
        "            edit_tok = tokenize(e)\n",
        "            sh_toks = tokenize(second_half)\n",
        "\n",
        "            fh_pad = list(pad_sequence(fh_toks, self.order, pad_left=True, left_pad_symbol='<s>'))\n",
        "            sh_pad = list(pad_sequence(sh_toks, self.order, pad_right=True, right_pad_symbol='</s>'))\n",
        "\n",
        "            edit_in_context = fh_pad[len(fh_pad) - self.order + 1:] +\\\n",
        "                                edit_tok + sh_pad[:self.order]\n",
        "            edits_with_context.append(list(everygrams(edit_in_context, max_len=self.order)))\n",
        "        \n",
        "        return edits_with_context\n",
        "\n",
        "    def predict(self, test_orig, test_edit):\n",
        "        # Assume tokenised sentence\n",
        "        test_edit_context_everygrams = self.extract_edit_context(test_orig, test_edit)\n",
        "\n",
        "        perps = self.get_perplexities(test_edit_context_everygrams)\n",
        "\n",
        "        np_perps = np.expand_dims(np.array(perps), axis=1)\n",
        "\n",
        "        return self.regressor.predict(np_perps)\n",
        "\n",
        "    def _get_perp(self, sentence):\n",
        "        return self.lm.perplexity(sentence)\n",
        "\n",
        "    def get_perplexities(self, sentences):\n",
        "        with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "            return pool.map(self._get_perp, sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxX8WpqS0mRD",
        "outputId": "c57d23f9-033c-49f5-c05a-d4b9fff8b902"
      },
      "source": [
        "# OD Trigram\n",
        "context_lfme3 = EditContextLMFunnyEstimator(3, train_original, train_raw, train_edit, train_grades)\n",
        "\n",
        "context_preds = context_lfme3.predict(val_raw, val_edit)\n",
        "model_performance(context_preds, val_grades, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Performance:\n",
            "\n",
            "| MSE: 0.33984 | RMSE: 0.58295 |\n",
            "\n",
            "| MSE: 0.33232 | RMSE: 0.57647 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(803.872178443305, 0.33231590675622363)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Unq64HYs0g",
        "outputId": "c3f42818-df9a-4627-de20-938643cb0285"
      },
      "source": [
        "# OD Bigram\r\n",
        "context_lfme2 = EditContextLMFunnyEstimator(2, train_original, train_raw, train_edit, train_grades)\r\n",
        "\r\n",
        "context_preds = context_lfme2.predict(val_raw, val_edit)\r\n",
        "model_performance(context_preds,  val_grades, True)\r\n",
        "\r\n",
        "# OD Unigram\r\n",
        "context_lfme1 = EditContextLMFunnyEstimator(1, train_original, train_raw, train_edit, train_grades)\r\n",
        "\r\n",
        "context_preds = context_lfme1.predict(val_raw, val_edit)\r\n",
        "model_performance(context_preds, val_grades, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Performance:\n",
            "\n",
            "| MSE: 0.34029 | RMSE: 0.58334 |\n",
            "\n",
            "| MSE: 0.33328 | RMSE: 0.57730 |\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.33915 | RMSE: 0.58237 |\n",
            "\n",
            "| MSE: 0.33339 | RMSE: 0.57740 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(806.4742345195133, 0.3333915810332838)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3o6JRYd2kFF",
        "outputId": "92d4dd83-8acb-4f55-b692-8282b043057a"
      },
      "source": [
        "print(context_lfme3.regressor.coef_)\n",
        "print(context_lfme3.regressor.intercept_)\n",
        "print(context_lfme2.regressor.coef_)\n",
        "print(context_lfme2.regressor.intercept_)\n",
        "print(context_lfme1.regressor.coef_)\n",
        "print(context_lfme1.regressor.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.43947639e-05]\n",
            "0.9798939828486125\n",
            "[-5.75712532e-06]\n",
            "0.9618051982754973\n",
            "[9.17320358e-07]\n",
            "0.9059682086772738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1eHqznh7wa0",
        "outputId": "081adc67-de93-4f62-ce0d-40b966150feb"
      },
      "source": [
        "# OD + EF Trigram\r\n",
        "print(\"Trigram\")\r\n",
        "comb_context_lfme3 = EditContextLMFunnyEstimator(3, combined_train_original, combined_train_raw, combined_train_edit, combined_train_grades)\r\n",
        "context_preds = comb_context_lfme3.predict(val_raw, val_edit)\r\n",
        "model_performance(context_preds, val_grades, True)\r\n",
        "print(comb_context_lfme3.regressor.coef_)\r\n",
        "print(comb_context_lfme3.regressor.intercept_)\r\n",
        "print()\r\n",
        "\r\n",
        "# OD + EF Bigram\r\n",
        "print(\"Bigram\")\r\n",
        "comb_context_lfme2 = EditContextLMFunnyEstimator(2, combined_train_original, combined_train_raw, combined_train_edit, combined_train_grades)\r\n",
        "context_preds = comb_context_lfme2.predict(val_raw, val_edit)\r\n",
        "model_performance(context_preds, val_grades, True)\r\n",
        "print(comb_context_lfme2.regressor.coef_)\r\n",
        "print(comb_context_lfme2.regressor.intercept_)\r\n",
        "print()\r\n",
        "\r\n",
        "# OD + EF Unigram\r\n",
        "print(\"Unigram\")\r\n",
        "comb_context_lfme1 = EditContextLMFunnyEstimator(1, combined_train_original, combined_train_raw, combined_train_edit, combined_train_grades)\r\n",
        "context_preds = comb_context_lfme1.predict(val_raw, val_edit)\r\n",
        "model_performance(context_preds, val_grades, True)\r\n",
        "print(comb_context_lfme1.regressor.coef_)\r\n",
        "print(comb_context_lfme1.regressor.intercept_)\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigram\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35514 | RMSE: 0.59593 |\n",
            "\n",
            "| MSE: 0.35593 | RMSE: 0.59660 |\n",
            "[-3.2094862e-06]\n",
            "1.1064255723083531\n",
            "\n",
            "Bigram\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35518 | RMSE: 0.59597 |\n",
            "\n",
            "| MSE: 0.35745 | RMSE: 0.59787 |\n",
            "[-1.68569998e-06]\n",
            "1.1021019208947336\n",
            "\n",
            "Unigram\n",
            "Train Performance:\n",
            "\n",
            "| MSE: 0.35296 | RMSE: 0.59410 |\n",
            "\n",
            "| MSE: 0.35778 | RMSE: 0.59814 |\n",
            "[6.49494333e-07]\n",
            "1.0581312525883806\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtrBjZRQT_8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26e8b2b-5d7c-4dd0-ed1f-a7debe20a937"
      },
      "source": [
        "for _ in range(5):\n",
        "  print(comb_context_lfme3.lm.generate(15, text_seed='<s>'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sparks', 'Outrage', 'Across', 'India', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['Center', 'Clients', 'Rely', 'On', 'Medicaid', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "['strange', 'collection', 'of', 'location', 'data', 'even', 'when', 'told', 'not', 'to', '-', '9to5Mac', '</s>', '</s>', '</s>']\n",
            "['executive', 'cooperating', 'in', 'US', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "[',', 'exit', 'polls', 'show', 'Trump', ',', 'Putin', 'to', 'hold', 'presidency', '.', '</s>', '</s>', '</s>', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApJa15BpWeUA",
        "outputId": "55b65a63-e47d-470e-86bc-8471d51c3f8f"
      },
      "source": [
        "for _ in range(5):\r\n",
        "  print(comb_context_lfme3.lm.generate(10, text_seed='<s>'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dozens', 'dead', 'in', 'possible', 'gas', 'attack', 'in', 'Syria', '</s>', '</s>']\n",
            "['hiding', 'something', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "[\"'\", ':', 'midwestern', 'workers', 'savaged', 'by', 'Trump', '’s', 'indefensible', 'defense']\n",
            "['<s>', '<s>', 'Alexandria', 'Ocasio-Cortez', ':', 'Trump', 'and', 'the', 'art', 'of']\n",
            "['fight', 'climate', 'crisis', ':', 'Death', 'toll', 'rises', 'after', 'bombs', 'target']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZeztVc_idb"
      },
      "source": [
        "## **Perplexity results**\n",
        "**LM on uneditted, do linear regression between perplexities of editted headlines and funniness:**\n",
        "\n",
        "Trigram:\n",
        "\n",
        " - Model training score:  0.006487269059277545\n",
        " - MSE: 0.35\n",
        " - RMSE: 0.59\n",
        " - Perplexity coefficient: [-0.00014175]\n",
        " - Intercept: 1.094308667001804\n",
        "\n",
        "Bigram:\n",
        "\n",
        " - Model training score:  0.00802764275019574\n",
        " - MSE: 0.34\n",
        " - RMSE: 0.58\n",
        " - Perplexity coefficient: [-7.66573025e-05]\n",
        " - Intercept: 1.0641348931340255\n",
        "\n",
        "Unigram:\n",
        "\n",
        " - Model training score:  0.00489278156461237\n",
        " - MSE: 0.33\n",
        " - RMSE: 0.58\n",
        " - Perplexity coefficient: [-1.48554245e-05]\n",
        " - Intercept: 0.9824355881975723\n",
        "\n",
        "**Same LM, now fit perplexities just from words around edit**\n",
        "\n",
        "Trigram:\n",
        "\n",
        " - Model training score:  0.0022555031224428257\n",
        " - MSE: 0.33\n",
        " - RMSE: 0.58\n",
        " - Perplexity coefficient: [-1.43947639e-05]\n",
        " - Intercept: 0.9798939828486125\n",
        "\n",
        "Bigram:\n",
        "\n",
        " - Model training score:  0.0009207103129988958\n",
        " - MSE: 0.33\n",
        " - RMSE: 0.58\n",
        " - Perplexity coefficient: [-5.75712532e-06]\n",
        " - Intercept: 0.9618051982754973\n",
        "\n",
        "Unigram:\n",
        "\n",
        " - Model training score:  0.00426709488810717\n",
        " - MSE: 0.33\n",
        " - RMSE: 0.58\n",
        " - Perplexity coefficient: [9.17320358e-07]\n",
        " - Intercept: 0.9059682086772738\n",
        "\n",
        "**LM on uneditted + funlines uneditted, do linear regression between perplexities of editted headlines and funniness:**\n",
        "\n",
        "Trigram:\n",
        "\n",
        " - Model training score:  0.0008570347665806112\n",
        " - MSE: 0.37\n",
        " - RMSE: 0.61\n",
        " - [2.71052366e-05]\n",
        " - 1.0417056013230948\n",
        "\n",
        "Bigram:\n",
        "\n",
        " - Model training score:  0.00021799908115582856\n",
        " - MSE: 0.35\n",
        " - RMSE: 0.60\n",
        " - Perplexity coefficient: [-7.37558235e-06]\n",
        " - Intercept: 1.1093102757298205\n",
        "\n",
        "Unigram:\n",
        " \n",
        " - Model training score:  0.0024696745781895846\n",
        " - MSE: 0.35\n",
        " - RMSE: 0.60\n",
        " - Perplexity coefficient: [-7.73702134e-06]\n",
        " - Intercept: 1.121725776362903\n",
        "\n",
        "\n",
        "**Same extended LM, but  LM, now fit perplexities just from words around edit**\n",
        "\n",
        "Trigram: \n",
        " - Model training score:  0.00035018215042048606\n",
        " - MSE: 0.36\n",
        " - RMSE: 0.60\n",
        " - Perplexity coeffecient: [-3.2094862e-06]\n",
        " - Intercept: 1.1064255723083531\n",
        "\n",
        "Bigram: \n",
        " - Model training score:  0.00022490457320711865\n",
        " - MSE: 0.36\n",
        " - RMSE: 0.60\n",
        " - Perplexity coeffecient: [-1.68569998e-06]\n",
        " - Intercept: 1.1021019208947336\n",
        "\n",
        "Unigram: \n",
        " - Model training score:  0.006474407232901269\n",
        " - MSE: 0.36\n",
        " - RMSE: 0.60\n",
        " - Perplexity coeffecient: [6.49494333e-07]\n",
        " - Intercept: 1.0581312525883806\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezftV-C1ES9t"
      },
      "source": [
        "# Approach 3: Averaging across POS/NER tag\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSmYx7hMvgUA"
      },
      "source": [
        "\r\n",
        "class POSTagFunninessPredictor:\r\n",
        "\r\n",
        "    def __init__(self, data_edited, data_orig, data_edit, data_grades, tagset, min_count, use_ner):\r\n",
        "\r\n",
        "        self.avg_score = np.mean(data_grades)\r\n",
        "        self.tagset = tagset\r\n",
        "        self.use_ner = use_ner\r\n",
        "\r\n",
        "        tag_counts = {}\r\n",
        "        tag_score_sums = {}\r\n",
        "\r\n",
        "        for i in range(len(data_edited)):\r\n",
        "            s = data_edited[i]\r\n",
        "            s_o = data_orig[i]\r\n",
        "            e = data_edit[i]\r\n",
        "            g = data_grades[i]\r\n",
        "\r\n",
        "            t = self.get_edit_tag(s, s_o, e)\r\n",
        "\r\n",
        "            if t in tag_counts:\r\n",
        "                tag_counts[t] += 1\r\n",
        "                tag_score_sums[t] += g\r\n",
        "            else:\r\n",
        "                tag_counts[t] = 1\r\n",
        "                tag_score_sums[t] = g\r\n",
        "\r\n",
        "        self.tag_counts = tag_counts\r\n",
        "        self.avg_tag_scores = {}\r\n",
        "\r\n",
        "        for t in tag_counts.keys():\r\n",
        "            if tag_counts[t] >= min_count:\r\n",
        "                self.avg_tag_scores[t] = tag_score_sums[t] / tag_counts[t]\r\n",
        "\r\n",
        "    def get_edit_tag(self, s, s_o, e):\r\n",
        "\r\n",
        "        # Remove punctuation that messes up truecaser\r\n",
        "        s = re.sub(r'\\s(-|\\||~)\\s', ' ', s)\r\n",
        "        s_o = re.sub(r'\\s(-|\\|)\\s', ' ', s_o)\r\n",
        "\r\n",
        "        s = get_true_case(s)\r\n",
        "        s_o = get_true_case(s_o)\r\n",
        "\r\n",
        "        tagged_s = pos_tag(word_tokenize(get_true_case(s)), tagset=self.tagset)\r\n",
        "        tagged_s_o = pos_tag(word_tokenize(get_true_case(s_o)), tagset=self.tagset)\r\n",
        "\r\n",
        "        conll_tag_s = tree2conlltags(ne_chunk(tagged_s))\r\n",
        "        conll_tag_s_o = tree2conlltags(ne_chunk(tagged_s_o))\r\n",
        "\r\n",
        "        if len(conll_tag_s_o) < len(conll_tag_s):\r\n",
        "            conll_tag_s_o += [('', 'FAKE', 'O')] * (len(conll_tag_s) - len(conll_tag_s_o))\r\n",
        "\r\n",
        "        found = False\r\n",
        "        edit_tag = 'UNK' # For unknown\r\n",
        "        for (w, t, conll_t), (w_o, _, _) in zip(conll_tag_s, conll_tag_s_o):\r\n",
        "\r\n",
        "            if w == w_o:\r\n",
        "                continue\r\n",
        "            if w.lower() == e.lower():\r\n",
        "                if found:\r\n",
        "                    print(\"WARNING - multiple occurences of edit word\")\r\n",
        "                    print(s)\r\n",
        "                    print(e)\r\n",
        "                    print()\r\n",
        "                else:\r\n",
        "                    if self.use_ner and conll_t != 'O':\r\n",
        "                        edit_tag = conll_t[2:]\r\n",
        "                    else:\r\n",
        "                        edit_tag = t\r\n",
        "                    found = True\r\n",
        "        if not found:\r\n",
        "            print(\"ERROR - could not find edit word\")\r\n",
        "            print(s)\r\n",
        "            print(s_o)\r\n",
        "            print(e)\r\n",
        "            print(tagged_s)\r\n",
        "            print(tagged_s_o)\r\n",
        "            print(ne_chunk(tagged_s))\r\n",
        "            print(ne_chunk(tagged_s_o))\r\n",
        "            print(conll_tag_s)\r\n",
        "            print(conll_tag_s_o)\r\n",
        "            print()\r\n",
        "        return edit_tag\r\n",
        "\r\n",
        "    def predict(self, sentence, sentence_orig, edit):\r\n",
        "        t = self.get_edit_tag(sentence, sentence_orig, edit)\r\n",
        "\r\n",
        "        if t in self.avg_tag_scores:\r\n",
        "            return self.avg_tag_scores[t]\r\n",
        "        else:\r\n",
        "            return self.avg_score\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6UoOSABFiZe7",
        "outputId": "4394d755-1cca-4636-b653-10b20d35201e"
      },
      "source": [
        "re.sub(r'\\s(-|\\||~)\\s', ' ', ' test ~ bbc')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' test bbc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMYSGtjAOse8",
        "outputId": "aabd2728-60c0-40d8-a377-fdf4c308908d"
      },
      "source": [
        "\r\n",
        "def test_pos_predictor(all_data=False, tagset=None, min_count=0, use_ner=False):\r\n",
        "    if all_data:\r\n",
        "        pos_predictor = POSTagFunninessPredictor(combined_train_edited,\r\n",
        "                                                 combined_train_original,\r\n",
        "                                                 combined_train_edit,\r\n",
        "                                                 combined_train_grades,\r\n",
        "                                                 tagset=tagset,\r\n",
        "                                                 min_count=min_count,\r\n",
        "                                                 use_ner=use_ner)\r\n",
        "    else:\r\n",
        "        pos_predictor = POSTagFunninessPredictor(train_edited, train_original,\r\n",
        "                                                 train_edit, train_grades,\r\n",
        "                                                 tagset=tagset,\r\n",
        "                                                 min_count=min_count,\r\n",
        "                                                 use_ner=use_ner)\r\n",
        "\r\n",
        "    print(pos_predictor.tag_counts)\r\n",
        "\r\n",
        "\r\n",
        "    val_res = [pos_predictor.predict(val_edited[i], val_original[i], val_edit[i])\r\n",
        "               for i in range(len(val_edited))]\r\n",
        "    val_res = np.array(val_res)\r\n",
        "    print(\"Validation performance:\")\r\n",
        "    print()\r\n",
        "    model_performance(val_res, val_grades, True)\r\n",
        "    print()\r\n",
        "\r\n",
        "    test_res = [pos_predictor.predict(test_edited[i], test_original[i], test_edit[i])\r\n",
        "               for i in range(len(test_edited))]\r\n",
        "    test_res = np.array(test_res)\r\n",
        "    print(\"Test performance:\")\r\n",
        "    print()\r\n",
        "    model_performance(test_res, test_grades, True)\r\n",
        "    print()\r\n",
        "    print()\r\n",
        "    print()\r\n",
        "\r\n",
        "print(\"Normal train, PTB tagset\")\r\n",
        "test_pos_predictor(use_ner=True)\r\n",
        "print(\"Normal train, PTB tagset, min_count=10\")\r\n",
        "test_pos_predictor(use_ner=True, min_count=10)\r\n",
        "print(\"Normal train, PTB tagset, min_count=50\")\r\n",
        "test_pos_predictor(use_ner=True, min_count=50)\r\n",
        "print(\"Normal train, Universal tagset\")\r\n",
        "test_pos_predictor(use_ner=True, tagset='universal')\r\n",
        "print(\"Normal train, Universal tagset, min_count=10\")\r\n",
        "test_pos_predictor(use_ner=True, tagset='universal', min_count=10)\r\n",
        "\r\n",
        "\r\n",
        "## Train on normal data + Funlines\r\n",
        "\r\n",
        "# print(\"All train, PTB tagset\")\r\n",
        "# test_pos_predictor(all_data=True)\r\n",
        "# print(\"All train, PTB tagset, min_count=10\")\r\n",
        "# test_pos_predictor(all_data=True, min_count=10)\r\n",
        "# print(\"All train, PTB tagset, min_count=50\")\r\n",
        "# test_pos_predictor(all_data=True, min_count=50)\r\n",
        "# print(\"All train, Universal tagset\")\r\n",
        "# test_pos_predictor(all_data=True, tagset='universal')\r\n",
        "# print(\"All train, Universal tagset, min_count=10\")\r\n",
        "# test_pos_predictor(all_data=True, tagset='universal', min_count=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal train, PTB tagset\n",
            "{'NNS': 1758, 'VBG': 358, 'NN': 3723, 'VBP': 120, 'NNP': 497, 'PERSON': 748, 'VB': 590, 'VBD': 162, 'GPE': 488, 'JJ': 405, 'VBZ': 248, 'ORGANIZATION': 346, 'PRP': 24, 'WDT': 1, 'IN': 14, 'VBN': 40, 'RB': 45, 'DT': 9, 'NNPS': 22, 'FACILITY': 15, 'WP': 2, 'RP': 7, 'GSP': 4, 'FW': 5, 'LOCATION': 2, 'CD': 5, 'JJR': 4, 'MD': 1, 'PRP$': 4, 'CC': 1, 'WRB': 1, 'PDT': 1, 'JJS': 2}\n",
            "Validation performance:\n",
            "\n",
            "| MSE: 0.33274 | RMSE: 0.57684 |\n",
            "\n",
            "Test performance:\n",
            "\n",
            "| MSE: 0.32875 | RMSE: 0.57337 |\n",
            "\n",
            "\n",
            "\n",
            "Normal train, PTB tagset, min_count=10\n",
            "{'NNS': 1758, 'VBG': 358, 'NN': 3723, 'VBP': 120, 'NNP': 497, 'PERSON': 748, 'VB': 590, 'VBD': 162, 'GPE': 488, 'JJ': 405, 'VBZ': 248, 'ORGANIZATION': 346, 'PRP': 24, 'WDT': 1, 'IN': 14, 'VBN': 40, 'RB': 45, 'DT': 9, 'NNPS': 22, 'FACILITY': 15, 'WP': 2, 'RP': 7, 'GSP': 4, 'FW': 5, 'LOCATION': 2, 'CD': 5, 'JJR': 4, 'MD': 1, 'PRP$': 4, 'CC': 1, 'WRB': 1, 'PDT': 1, 'JJS': 2}\n",
            "Validation performance:\n",
            "\n",
            "| MSE: 0.33243 | RMSE: 0.57656 |\n",
            "\n",
            "Test performance:\n",
            "\n",
            "| MSE: 0.32830 | RMSE: 0.57297 |\n",
            "\n",
            "\n",
            "\n",
            "Normal train, PTB tagset, min_count=50\n",
            "{'NNS': 1758, 'VBG': 358, 'NN': 3723, 'VBP': 120, 'NNP': 497, 'PERSON': 748, 'VB': 590, 'VBD': 162, 'GPE': 488, 'JJ': 405, 'VBZ': 248, 'ORGANIZATION': 346, 'PRP': 24, 'WDT': 1, 'IN': 14, 'VBN': 40, 'RB': 45, 'DT': 9, 'NNPS': 22, 'FACILITY': 15, 'WP': 2, 'RP': 7, 'GSP': 4, 'FW': 5, 'LOCATION': 2, 'CD': 5, 'JJR': 4, 'MD': 1, 'PRP$': 4, 'CC': 1, 'WRB': 1, 'PDT': 1, 'JJS': 2}\n",
            "Validation performance:\n",
            "\n",
            "| MSE: 0.33288 | RMSE: 0.57696 |\n",
            "\n",
            "Test performance:\n",
            "\n",
            "| MSE: 0.32845 | RMSE: 0.57311 |\n",
            "\n",
            "\n",
            "\n",
            "Normal train, Universal tagset\n",
            "{'NOUN': 7494, 'VERB': 1519, 'ADJ': 437, 'GPE': 30, 'ORGANIZATION': 48, 'PRON': 30, 'DET': 11, 'ADP': 14, 'ADV': 46, 'PRT': 7, 'X': 5, 'PERSON': 5, 'NUM': 5, 'CONJ': 1}\n",
            "Validation performance:\n",
            "\n",
            "| MSE: 0.33440 | RMSE: 0.57827 |\n",
            "\n",
            "Test performance:\n",
            "\n",
            "| MSE: 0.33141 | RMSE: 0.57569 |\n",
            "\n",
            "\n",
            "\n",
            "Normal train, Universal tagset, min_count=10\n",
            "{'NOUN': 7494, 'VERB': 1519, 'ADJ': 437, 'GPE': 30, 'ORGANIZATION': 48, 'PRON': 30, 'DET': 11, 'ADP': 14, 'ADV': 46, 'PRT': 7, 'X': 5, 'PERSON': 5, 'NUM': 5, 'CONJ': 1}\n",
            "Validation performance:\n",
            "\n",
            "| MSE: 0.33487 | RMSE: 0.57868 |\n",
            "\n",
            "Test performance:\n",
            "\n",
            "| MSE: 0.33079 | RMSE: 0.57515 |\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaHqiemjqyXt"
      },
      "source": [
        "POS Results:\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, PTB tagset**\r\n",
        "\r\n",
        "{'NNS': 1797, 'VBG': 358, 'NN': 3805, 'VBP': 120, 'NNP': 1932, 'VB': 590, 'VBD': 162, 'JJ': 434, 'VBZ': 248, 'PRP': 24, 'WDT': 1, 'IN': 14, 'VBN': 40, 'RB': 45, 'NNPS': 39, 'DT': 9, 'WP': 2, 'RP': 7, 'FW': 5, 'CD': 5, 'JJR': 4, 'MD': 1, 'PRP\\$': 4, 'CC': 1, 'JJS': 3, 'WRB': 1, 'PDT': 1}\r\n",
        "\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33193 | RMSE: 0.57613 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32910 | RMSE: 0.57367 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, PTB tagset, min_count=10**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33223 | RMSE: 0.57639 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32847 | RMSE: 0.57312 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, PTB tagset, min_count=50**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33265 | RMSE: 0.57676 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32863 | RMSE: 0.57327 |\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, Universal tagset**\r\n",
        "\r\n",
        "{'NOUN': 7573, 'VERB': 1519, 'ADJ': 441, 'PRON': 30, 'DET': 11, 'ADP': 14, 'ADV': 46, 'PRT': 7, 'X': 5, 'NUM': 5, 'CONJ': 1}\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33452 | RMSE: 0.57838 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.33100 | RMSE: 0.57533 |\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, Universal tagset, min_count=10**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33489 | RMSE: 0.57870 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.33080 | RMSE: 0.57515 |\r\n",
        "\r\n",
        "\r\n",
        "**All train, PTB tagset**\r\n",
        "\r\n",
        "{'NNS': 3301, 'VBG': 752, 'NN': 6412, 'VBP': 203, 'NNP': 4285, 'VB': 1034, 'VBD': 335, 'JJ': 745, 'VBZ': 430, 'PRP': 54, 'WDT': 1, 'IN': 21, 'VBN': 68, 'RB': 70, 'NNPS': 97, 'DT': 27, 'WP': 2, 'RP': 7, 'FW': 7, 'CD': 13, 'JJR': 10, 'MD': 1, 'PRP\\$': 8, 'CC': 3, 'JJS': 8, 'WRB': 2, 'PDT': 1, 'POS': 1, 'UNK': 1, '\\$': 1}\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.35459 | RMSE: 0.59547 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.35155 | RMSE: 0.59291 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**All train, PTB tagset, min_count=10**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.35497 | RMSE: 0.59579 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.35133 | RMSE: 0.59273 |\r\n",
        "\r\n",
        "\r\n",
        "**All train, PTB tagset, min_count=50**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.35540 | RMSE: 0.59615 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.35111 | RMSE: 0.59255 |\r\n",
        "\r\n",
        "\r\n",
        "**All train, Universal tagset**\r\n",
        "\r\n",
        "{'NOUN': 14095, 'VERB': 2823, 'ADJ': 763, 'PRON': 64, 'DET': 29, 'ADP': 21, 'ADV': 72, 'PRT': 8, 'X': 7, 'NUM': 13, 'CONJ': 3, 'UNK': 1, '.': 1}\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.35973 | RMSE: 0.59977 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.35359 | RMSE: 0.59463 |\r\n",
        "\r\n",
        "\r\n",
        "**All train, Universal tagset, min_count=10**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.35984 | RMSE: 0.59987 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.35349 | RMSE: 0.59455 |\r\n",
        "\r\n",
        "**Normal train, PTB tagset + NER**\r\n",
        "\r\n",
        "{'NNS': 1758, 'VBG': 358, 'NN': 3723, 'VBP': 120, 'NNP': 497, 'PERSON': 748, 'VB': 590, 'VBD': 162, 'GPE': 488, 'JJ': 405, 'VBZ': 248, 'ORGANIZATION': 346, 'PRP': 24, 'WDT': 1, 'IN': 14, 'VBN': 40, 'RB': 45, 'DT': 9, 'NNPS': 22, 'FACILITY': 15, 'WP': 2, 'RP': 7, 'GSP': 4, 'FW': 5, 'LOCATION': 2, 'CD': 5, 'JJR': 4, 'MD': 1, 'PRP\\$': 4, 'CC': 1, 'WRB': 1, 'PDT': 1, 'JJS': 2}\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33274 | RMSE: 0.57684 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32875 | RMSE: 0.57337 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, PTB tagset + NER, min_count=10**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33243 | RMSE: 0.57656 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32830 | RMSE: 0.57297 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, PTB tagset + NER, min_count=50**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33288 | RMSE: 0.57696 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.32845 | RMSE: 0.57311 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, Universal tagset + NER**\r\n",
        "\r\n",
        "{'NOUN': 7494, 'VERB': 1519, 'ADJ': 437, 'GPE': 30, 'ORGANIZATION': 48, 'PRON': 30, 'DET': 11, 'ADP': 14, 'ADV': 46, 'PRT': 7, 'X': 5, 'PERSON': 5, 'NUM': 5, 'CONJ': 1}\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33440 | RMSE: 0.57827 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.33141 | RMSE: 0.57569 |\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Normal train, Universal tagset + NER, min_count=10**\r\n",
        "\r\n",
        "Validation performance:\r\n",
        "\r\n",
        "| MSE: 0.33487 | RMSE: 0.57868 |\r\n",
        "\r\n",
        "Test performance:\r\n",
        "\r\n",
        "| MSE: 0.33079 | RMSE: 0.57515 |\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQPSUbAZp6SL"
      },
      "source": [
        "# Baseline (predict mean of train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZBs5uI4p6SL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1d55d1-db07-4244-c6aa-15a2a4bebb54"
      },
      "source": [
        "# Baseline for the task\n",
        "\n",
        "train_mean = np.mean(train_grades)\n",
        "\n",
        "train_pred_baseline = torch.zeros(len(train_grades)) + train_mean\n",
        "val_pred_baseline = torch.zeros(len(val_grades)) + train_mean\n",
        "test_pred_baseline = torch.zeros(len(test_grades)) + train_mean\n",
        "\n",
        "print(\"Mean: \", train_mean)\n",
        "\n",
        "print(\"\\nBaseline train performance:\")\n",
        "sse, mse = model_performance(train_pred_baseline, train_grades, True)\n",
        "\n",
        "print(\"\\nBaseline validation performance:\")\n",
        "sse, mse = model_performance(val_pred_baseline, val_grades, True)\n",
        "\n",
        "print(\"\\nBaseline validation performance:\")\n",
        "sse, mse = model_performance(test_pred_baseline, test_grades, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean:  0.9355712114932938\n",
            "\n",
            "Baseline train performance:\n",
            "| MSE: 0.34060 | RMSE: 0.58361 |\n",
            "\n",
            "Baseline validation performance:\n",
            "| MSE: 0.33455 | RMSE: 0.57840 |\n",
            "\n",
            "Baseline validation performance:\n",
            "| MSE: 0.33029 | RMSE: 0.57471 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foo97MCc3Zpd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}