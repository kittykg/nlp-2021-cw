{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twelve-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "homeless-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_UNEDITED = True\n",
    "ADD_TF = True\n",
    "ADD_TF_UNEDITED = True\n",
    "VAL_ADD_UNEDITED = False\n",
    "\n",
    "PRETRAINED_WEIGHTS = 'roberta-base'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VAL_STEPS = 100\n",
    "\n",
    "MODEL_PATH = './models/funniness_roberta_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breeding-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "meaningful-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "tf_df = pd.read_csv('data/task-1/train_funlines.csv')\n",
    "dev_df = pd.read_csv('data/task-1/dev.csv')\n",
    "test_df = pd.read_csv('data/task-1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinate-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    raw_data = df['original']\n",
    "    edit_data = df['edit']\n",
    "    original_data = pd.Series([re.sub('<|\\/>', '', s) for s in raw_data])\n",
    "    edited_data = pd.Series([re.sub('<.*\\/>', e, s) for s, e in zip(raw_data, edit_data)])\n",
    "    grade_data = df['meanGrade']\n",
    "    return original_data, edited_data, grade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prostate-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original training set\n",
    "train_unedited_data, train_edited_data, train_edited_score = extract_data(train_df)\n",
    "train_unedited_score = pd.Series([0] * len(train_unedited_data))\n",
    "\n",
    "# Funline for training set\n",
    "tf_unedited_data, tf_edited_data, tf_edited_score = extract_data(tf_df)\n",
    "tf_unedited_score = pd.Series([0] * len(tf_unedited_data))\n",
    "\n",
    "# Validation (dev) set\n",
    "val_unedited_data, val_edited_data, val_edited_score = extract_data(dev_df)\n",
    "val_unedited_score = pd.Series([0] * len(val_unedited_data))\n",
    "\n",
    "# Test set\n",
    "_, test_edited_data, test_edited_score = extract_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = train_edited_data\n",
    "full_train_score = train_edited_score\n",
    "\n",
    "full_val_data = val_edited_data\n",
    "full_val_score = val_edited_score\n",
    "\n",
    "if ADD_UNEDITED:\n",
    "    full_train_data = full_train_data.append(train_unedited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(train_unedited_score, ignore_index=True)\n",
    "\n",
    "if ADD_TF:\n",
    "    full_train_data = full_train_data.append(tf_edited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(tf_edited_score, ignore_index=True)\n",
    "\n",
    "if ADD_TF_UNEDITED:\n",
    "    full_train_data = full_train_data.append(tf_unedited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(tf_unedited_score, ignore_index=True)\n",
    "\n",
    "if VAL_ADD_UNEDITED:\n",
    "    full_val_data = full_val_data.append(val_unedited_data, ignore_index=True)\n",
    "    full_val_score = full_val_score.append(val_unedited_score, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hundred-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = pd.concat([full_train_data, full_train_score], axis=1)\n",
    "full_train_dataset.columns = ['text', 'score']\n",
    "\n",
    "full_val_dataset = pd.concat([full_val_data, full_val_score], axis=1)\n",
    "full_val_dataset.columns = ['text', 'score']\n",
    "\n",
    "full_test_dataset = pd.concat([test_edited_data, test_edited_score], axis=1)\n",
    "full_test_dataset.columns = ['text', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worthy-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task1Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.x_train = input_set['text']\n",
    "        self.y_train = input_set['score']\n",
    "        \n",
    "    def collate_fn_padd(self, batch):\n",
    "        batch_sentences = [s['text'] for s in batch]\n",
    "        batch_scores = [s['score'] for s in batch]\n",
    "        encodings = self.tokenizer(batch_sentences,\n",
    "                                   return_tensors='pt',\n",
    "                                   padding=True,\n",
    "                                   truncation=True,\n",
    "                                   max_length=128)\n",
    "        encodings['score'] = torch.tensor(batch_scores)\n",
    "        return encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = {\n",
    "            'text': self.x_train.iloc[i],\n",
    "            'score': self.y_train.iloc[i]\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affecting-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Task1Dataset(tokenizer, full_train_dataset)\n",
    "val_dataset = Task1Dataset(tokenizer, full_val_dataset)\n",
    "\n",
    "print(\"Datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collective-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, PreTrainedModel\n",
    "\n",
    "\n",
    "class RobertaRegressor(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = RobertaModel(config)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return self.linear(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extended-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class RobertaRegressorTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop('score')\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss_fn = nn.MSELoss().to(device)\n",
    "        loss = loss_fn(outputs.view(-1), labels.to(device=device, dtype=torch.float))\n",
    "\n",
    "        return (loss.sqrt(), outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "permanent-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaRegressor were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.bert.embeddings.word_embeddings.weight', 'roberta.bert.embeddings.position_embeddings.weight', 'roberta.bert.embeddings.token_type_embeddings.weight', 'roberta.bert.embeddings.LayerNorm.weight', 'roberta.bert.embeddings.LayerNorm.bias', 'roberta.bert.encoder.layer.0.attention.self.query.weight', 'roberta.bert.encoder.layer.0.attention.self.query.bias', 'roberta.bert.encoder.layer.0.attention.self.key.weight', 'roberta.bert.encoder.layer.0.attention.self.key.bias', 'roberta.bert.encoder.layer.0.attention.self.value.weight', 'roberta.bert.encoder.layer.0.attention.self.value.bias', 'roberta.bert.encoder.layer.0.attention.output.dense.weight', 'roberta.bert.encoder.layer.0.attention.output.dense.bias', 'roberta.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.0.intermediate.dense.weight', 'roberta.bert.encoder.layer.0.intermediate.dense.bias', 'roberta.bert.encoder.layer.0.output.dense.weight', 'roberta.bert.encoder.layer.0.output.dense.bias', 'roberta.bert.encoder.layer.0.output.LayerNorm.weight', 'roberta.bert.encoder.layer.0.output.LayerNorm.bias', 'roberta.bert.encoder.layer.1.attention.self.query.weight', 'roberta.bert.encoder.layer.1.attention.self.query.bias', 'roberta.bert.encoder.layer.1.attention.self.key.weight', 'roberta.bert.encoder.layer.1.attention.self.key.bias', 'roberta.bert.encoder.layer.1.attention.self.value.weight', 'roberta.bert.encoder.layer.1.attention.self.value.bias', 'roberta.bert.encoder.layer.1.attention.output.dense.weight', 'roberta.bert.encoder.layer.1.attention.output.dense.bias', 'roberta.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.1.intermediate.dense.weight', 'roberta.bert.encoder.layer.1.intermediate.dense.bias', 'roberta.bert.encoder.layer.1.output.dense.weight', 'roberta.bert.encoder.layer.1.output.dense.bias', 'roberta.bert.encoder.layer.1.output.LayerNorm.weight', 'roberta.bert.encoder.layer.1.output.LayerNorm.bias', 'roberta.bert.encoder.layer.2.attention.self.query.weight', 'roberta.bert.encoder.layer.2.attention.self.query.bias', 'roberta.bert.encoder.layer.2.attention.self.key.weight', 'roberta.bert.encoder.layer.2.attention.self.key.bias', 'roberta.bert.encoder.layer.2.attention.self.value.weight', 'roberta.bert.encoder.layer.2.attention.self.value.bias', 'roberta.bert.encoder.layer.2.attention.output.dense.weight', 'roberta.bert.encoder.layer.2.attention.output.dense.bias', 'roberta.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.2.intermediate.dense.weight', 'roberta.bert.encoder.layer.2.intermediate.dense.bias', 'roberta.bert.encoder.layer.2.output.dense.weight', 'roberta.bert.encoder.layer.2.output.dense.bias', 'roberta.bert.encoder.layer.2.output.LayerNorm.weight', 'roberta.bert.encoder.layer.2.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.attention.self.query.weight', 'roberta.bert.encoder.layer.3.attention.self.query.bias', 'roberta.bert.encoder.layer.3.attention.self.key.weight', 'roberta.bert.encoder.layer.3.attention.self.key.bias', 'roberta.bert.encoder.layer.3.attention.self.value.weight', 'roberta.bert.encoder.layer.3.attention.self.value.bias', 'roberta.bert.encoder.layer.3.attention.output.dense.weight', 'roberta.bert.encoder.layer.3.attention.output.dense.bias', 'roberta.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.intermediate.dense.weight', 'roberta.bert.encoder.layer.3.intermediate.dense.bias', 'roberta.bert.encoder.layer.3.output.dense.weight', 'roberta.bert.encoder.layer.3.output.dense.bias', 'roberta.bert.encoder.layer.3.output.LayerNorm.weight', 'roberta.bert.encoder.layer.3.output.LayerNorm.bias', 'roberta.bert.encoder.layer.4.attention.self.query.weight', 'roberta.bert.encoder.layer.4.attention.self.query.bias', 'roberta.bert.encoder.layer.4.attention.self.key.weight', 'roberta.bert.encoder.layer.4.attention.self.key.bias', 'roberta.bert.encoder.layer.4.attention.self.value.weight', 'roberta.bert.encoder.layer.4.attention.self.value.bias', 'roberta.bert.encoder.layer.4.attention.output.dense.weight', 'roberta.bert.encoder.layer.4.attention.output.dense.bias', 'roberta.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.4.intermediate.dense.weight', 'roberta.bert.encoder.layer.4.intermediate.dense.bias', 'roberta.bert.encoder.layer.4.output.dense.weight', 'roberta.bert.encoder.layer.4.output.dense.bias', 'roberta.bert.encoder.layer.4.output.LayerNorm.weight', 'roberta.bert.encoder.layer.4.output.LayerNorm.bias', 'roberta.bert.encoder.layer.5.attention.self.query.weight', 'roberta.bert.encoder.layer.5.attention.self.query.bias', 'roberta.bert.encoder.layer.5.attention.self.key.weight', 'roberta.bert.encoder.layer.5.attention.self.key.bias', 'roberta.bert.encoder.layer.5.attention.self.value.weight', 'roberta.bert.encoder.layer.5.attention.self.value.bias', 'roberta.bert.encoder.layer.5.attention.output.dense.weight', 'roberta.bert.encoder.layer.5.attention.output.dense.bias', 'roberta.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.5.intermediate.dense.weight', 'roberta.bert.encoder.layer.5.intermediate.dense.bias', 'roberta.bert.encoder.layer.5.output.dense.weight', 'roberta.bert.encoder.layer.5.output.dense.bias', 'roberta.bert.encoder.layer.5.output.LayerNorm.weight', 'roberta.bert.encoder.layer.5.output.LayerNorm.bias', 'roberta.bert.encoder.layer.6.attention.self.query.weight', 'roberta.bert.encoder.layer.6.attention.self.query.bias', 'roberta.bert.encoder.layer.6.attention.self.key.weight', 'roberta.bert.encoder.layer.6.attention.self.key.bias', 'roberta.bert.encoder.layer.6.attention.self.value.weight', 'roberta.bert.encoder.layer.6.attention.self.value.bias', 'roberta.bert.encoder.layer.6.attention.output.dense.weight', 'roberta.bert.encoder.layer.6.attention.output.dense.bias', 'roberta.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.6.intermediate.dense.weight', 'roberta.bert.encoder.layer.6.intermediate.dense.bias', 'roberta.bert.encoder.layer.6.output.dense.weight', 'roberta.bert.encoder.layer.6.output.dense.bias', 'roberta.bert.encoder.layer.6.output.LayerNorm.weight', 'roberta.bert.encoder.layer.6.output.LayerNorm.bias', 'roberta.bert.encoder.layer.7.attention.self.query.weight', 'roberta.bert.encoder.layer.7.attention.self.query.bias', 'roberta.bert.encoder.layer.7.attention.self.key.weight', 'roberta.bert.encoder.layer.7.attention.self.key.bias', 'roberta.bert.encoder.layer.7.attention.self.value.weight', 'roberta.bert.encoder.layer.7.attention.self.value.bias', 'roberta.bert.encoder.layer.7.attention.output.dense.weight', 'roberta.bert.encoder.layer.7.attention.output.dense.bias', 'roberta.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.7.intermediate.dense.weight', 'roberta.bert.encoder.layer.7.intermediate.dense.bias', 'roberta.bert.encoder.layer.7.output.dense.weight', 'roberta.bert.encoder.layer.7.output.dense.bias', 'roberta.bert.encoder.layer.7.output.LayerNorm.weight', 'roberta.bert.encoder.layer.7.output.LayerNorm.bias', 'roberta.bert.encoder.layer.8.attention.self.query.weight', 'roberta.bert.encoder.layer.8.attention.self.query.bias', 'roberta.bert.encoder.layer.8.attention.self.key.weight', 'roberta.bert.encoder.layer.8.attention.self.key.bias', 'roberta.bert.encoder.layer.8.attention.self.value.weight', 'roberta.bert.encoder.layer.8.attention.self.value.bias', 'roberta.bert.encoder.layer.8.attention.output.dense.weight', 'roberta.bert.encoder.layer.8.attention.output.dense.bias', 'roberta.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.8.intermediate.dense.weight', 'roberta.bert.encoder.layer.8.intermediate.dense.bias', 'roberta.bert.encoder.layer.8.output.dense.weight', 'roberta.bert.encoder.layer.8.output.dense.bias', 'roberta.bert.encoder.layer.8.output.LayerNorm.weight', 'roberta.bert.encoder.layer.8.output.LayerNorm.bias', 'roberta.bert.encoder.layer.9.attention.self.query.weight', 'roberta.bert.encoder.layer.9.attention.self.query.bias', 'roberta.bert.encoder.layer.9.attention.self.key.weight', 'roberta.bert.encoder.layer.9.attention.self.key.bias', 'roberta.bert.encoder.layer.9.attention.self.value.weight', 'roberta.bert.encoder.layer.9.attention.self.value.bias', 'roberta.bert.encoder.layer.9.attention.output.dense.weight', 'roberta.bert.encoder.layer.9.attention.output.dense.bias', 'roberta.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.9.intermediate.dense.weight', 'roberta.bert.encoder.layer.9.intermediate.dense.bias', 'roberta.bert.encoder.layer.9.output.dense.weight', 'roberta.bert.encoder.layer.9.output.dense.bias', 'roberta.bert.encoder.layer.9.output.LayerNorm.weight', 'roberta.bert.encoder.layer.9.output.LayerNorm.bias', 'roberta.bert.encoder.layer.10.attention.self.query.weight', 'roberta.bert.encoder.layer.10.attention.self.query.bias', 'roberta.bert.encoder.layer.10.attention.self.key.weight', 'roberta.bert.encoder.layer.10.attention.self.key.bias', 'roberta.bert.encoder.layer.10.attention.self.value.weight', 'roberta.bert.encoder.layer.10.attention.self.value.bias', 'roberta.bert.encoder.layer.10.attention.output.dense.weight', 'roberta.bert.encoder.layer.10.attention.output.dense.bias', 'roberta.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.10.intermediate.dense.weight', 'roberta.bert.encoder.layer.10.intermediate.dense.bias', 'roberta.bert.encoder.layer.10.output.dense.weight', 'roberta.bert.encoder.layer.10.output.dense.bias', 'roberta.bert.encoder.layer.10.output.LayerNorm.weight', 'roberta.bert.encoder.layer.10.output.LayerNorm.bias', 'roberta.bert.encoder.layer.11.attention.self.query.weight', 'roberta.bert.encoder.layer.11.attention.self.query.bias', 'roberta.bert.encoder.layer.11.attention.self.key.weight', 'roberta.bert.encoder.layer.11.attention.self.key.bias', 'roberta.bert.encoder.layer.11.attention.self.value.weight', 'roberta.bert.encoder.layer.11.attention.self.value.bias', 'roberta.bert.encoder.layer.11.attention.output.dense.weight', 'roberta.bert.encoder.layer.11.attention.output.dense.bias', 'roberta.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.11.intermediate.dense.weight', 'roberta.bert.encoder.layer.11.intermediate.dense.bias', 'roberta.bert.encoder.layer.11.output.dense.weight', 'roberta.bert.encoder.layer.11.output.dense.bias', 'roberta.bert.encoder.layer.11.output.LayerNorm.weight', 'roberta.bert.encoder.layer.11.output.LayerNorm.bias', 'roberta.bert.pooler.dense.weight', 'roberta.bert.pooler.dense.bias', 'roberta.linear.1.weight', 'roberta.linear.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaRegressor.from_pretrained(PRETRAINED_WEIGHTS)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './experiment/roberta_regressor',\n",
    "    overwrite_output_dir = True,\n",
    "    learning_rate = 0.0001,\n",
    "    logging_steps = VAL_STEPS,\n",
    "    evaluation_strategy = 'steps',\n",
    "    label_names = ['score'],\n",
    "    eval_steps = VAL_STEPS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = NUM_EPOCHS,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_loss'\n",
    ")\n",
    "\n",
    "trainer = RobertaRegressorTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = train_dataset.collate_fn_padd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tutorial-incentive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3357' max='3357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3357/3357 24:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.789899</td>\n",
       "      <td>4.821800</td>\n",
       "      <td>501.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>4.815600</td>\n",
       "      <td>502.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.757003</td>\n",
       "      <td>4.835600</td>\n",
       "      <td>500.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.630757</td>\n",
       "      <td>4.828700</td>\n",
       "      <td>500.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.704679</td>\n",
       "      <td>4.843600</td>\n",
       "      <td>499.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>0.799423</td>\n",
       "      <td>4.832700</td>\n",
       "      <td>500.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>0.630530</td>\n",
       "      <td>4.847900</td>\n",
       "      <td>498.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.785751</td>\n",
       "      <td>4.836900</td>\n",
       "      <td>500.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.519400</td>\n",
       "      <td>0.635931</td>\n",
       "      <td>4.835800</td>\n",
       "      <td>500.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.760760</td>\n",
       "      <td>4.846500</td>\n",
       "      <td>499.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.608573</td>\n",
       "      <td>4.845000</td>\n",
       "      <td>499.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>4.833200</td>\n",
       "      <td>500.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.709645</td>\n",
       "      <td>4.837900</td>\n",
       "      <td>500.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.676934</td>\n",
       "      <td>4.834400</td>\n",
       "      <td>500.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.751740</td>\n",
       "      <td>4.845100</td>\n",
       "      <td>499.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.631070</td>\n",
       "      <td>4.835100</td>\n",
       "      <td>500.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.642076</td>\n",
       "      <td>4.839800</td>\n",
       "      <td>499.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.635353</td>\n",
       "      <td>4.834000</td>\n",
       "      <td>500.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.667934</td>\n",
       "      <td>4.842400</td>\n",
       "      <td>499.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.667759</td>\n",
       "      <td>4.833200</td>\n",
       "      <td>500.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.719280</td>\n",
       "      <td>4.840600</td>\n",
       "      <td>499.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.657659</td>\n",
       "      <td>4.836200</td>\n",
       "      <td>500.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.812614</td>\n",
       "      <td>4.849100</td>\n",
       "      <td>498.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.653850</td>\n",
       "      <td>4.833600</td>\n",
       "      <td>500.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.695216</td>\n",
       "      <td>4.854400</td>\n",
       "      <td>498.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.484100</td>\n",
       "      <td>0.721624</td>\n",
       "      <td>4.846500</td>\n",
       "      <td>499.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.730008</td>\n",
       "      <td>4.836900</td>\n",
       "      <td>500.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.724995</td>\n",
       "      <td>4.853700</td>\n",
       "      <td>498.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.673782</td>\n",
       "      <td>4.833900</td>\n",
       "      <td>500.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.699330</td>\n",
       "      <td>4.855100</td>\n",
       "      <td>498.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.694852</td>\n",
       "      <td>4.845000</td>\n",
       "      <td>499.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.699437</td>\n",
       "      <td>4.842900</td>\n",
       "      <td>499.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.663824</td>\n",
       "      <td>4.838400</td>\n",
       "      <td>499.959000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3357, training_loss=0.5076717078987601, metrics={'train_runtime': 1495.768, 'train_samples_per_second': 2.244, 'total_flos': 4593526796163504, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "united-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blessed-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaRegressor.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convenient-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(target, output):\n",
    "    sq_error = (output - target)**2\n",
    "    mse = torch.mean(sq_error)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse.item()\n",
    "\n",
    "\n",
    "def predict_funniness(text, tokenizer, model): \n",
    "    model.eval()\n",
    "    encodings = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    return model(**encodings)\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, dataset, testing=False):\n",
    "    with torch.no_grad():\n",
    "        text = dataset['text']\n",
    "        score = dataset['score'].tolist()\n",
    "        score = torch.Tensor(score)\n",
    "\n",
    "        pred = predict_funniness(text.tolist(), tokenizer, model)\n",
    "        mse, rmse = compute_metrics(score, pred.view(-1))\n",
    "        print(f'Eval on {\"test set\"if testing else \"val set\"} MSE:{mse} RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "twenty-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval on test set MSE:0.37407833337783813 RMSE: 0.6116194128990173\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, tokenizer, full_test_dataset, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-military",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
