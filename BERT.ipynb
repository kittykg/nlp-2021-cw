{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twelve-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "homeless-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_UNEDITED = True\n",
    "ADD_TF = False\n",
    "ADD_TF_UNEDITED = False\n",
    "VAL_ADD_UNEDITED = False\n",
    "\n",
    "PRETRAINED_WEIGHTS = 'bert-base-cased'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 6\n",
    "VAL_STEPS = 100\n",
    "\n",
    "MODEL_PATH = './models/funniness_bert_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breeding-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "meaningful-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "tf_df = pd.read_csv('data/task-1/train_funlines.csv')\n",
    "dev_df = pd.read_csv('data/task-1/dev.csv')\n",
    "test_df = pd.read_csv('data/task-1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinate-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    raw_data = df['original']\n",
    "    edit_data = df['edit']\n",
    "    original_data = pd.Series([re.sub('<|\\/>', '', s) for s in raw_data])\n",
    "    edited_data = pd.Series([re.sub('<.*\\/>', e, s) for s, e in zip(raw_data, edit_data)])\n",
    "    grade_data = df['meanGrade']\n",
    "    return original_data, edited_data, grade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prostate-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original training set\n",
    "train_unedited_data, train_edited_data, train_edited_score = extract_data(train_df)\n",
    "train_unedited_score = pd.Series([0] * len(train_unedited_data))\n",
    "\n",
    "# Funline for training set\n",
    "tf_unedited_data, tf_edited_data, tf_edited_score = extract_data(tf_df)\n",
    "tf_unedited_score = pd.Series([0] * len(tf_unedited_data))\n",
    "\n",
    "# Validation (dev) set\n",
    "val_unedited_data, val_edited_data, val_edited_score = extract_data(dev_df)\n",
    "val_unedited_score = pd.Series([0] * len(val_unedited_data))\n",
    "\n",
    "# Test set\n",
    "_, test_edited_data, test_edited_score = extract_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = train_edited_data\n",
    "full_train_score = train_edited_score\n",
    "\n",
    "full_val_data = val_edited_data\n",
    "full_val_score = val_edited_score\n",
    "\n",
    "if ADD_UNEDITED:\n",
    "    full_train_data = full_train_data.append(train_unedited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(train_unedited_score, ignore_index=True)\n",
    "\n",
    "if ADD_TF:\n",
    "    full_train_data = full_train_data.append(tf_edited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(tf_edited_score, ignore_index=True)\n",
    "\n",
    "if ADD_TF_UNEDITED:\n",
    "    full_train_data = full_train_data.append(tf_unedited_data, ignore_index=True)\n",
    "    full_train_score = full_train_score.append(tf_unedited_score, ignore_index=True)\n",
    "\n",
    "if VAL_ADD_UNEDITED:\n",
    "    full_val_data = full_val_data.append(val_unedited_data, ignore_index=True)\n",
    "    full_val_score = full_val_score.append(val_unedited_score, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hundred-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = pd.concat([full_train_data, full_train_score], axis=1)\n",
    "full_train_dataset.columns = ['text', 'score']\n",
    "\n",
    "full_val_dataset = pd.concat([full_val_data, full_val_score], axis=1)\n",
    "full_val_dataset.columns = ['text', 'score']\n",
    "\n",
    "full_test_dataset = pd.concat([test_edited_data, test_edited_score], axis=1)\n",
    "full_test_dataset.columns = ['text', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worthy-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task1Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.x_train = input_set['text']\n",
    "        self.y_train = input_set['score']\n",
    "        \n",
    "    def collate_fn_padd(self, batch):\n",
    "        batch_sentences = [s['text'] for s in batch]\n",
    "        batch_scores = [s['score'] for s in batch]\n",
    "        encodings = self.tokenizer(batch_sentences,\n",
    "                                   return_tensors='pt',\n",
    "                                   padding=True,\n",
    "                                   truncation=True,\n",
    "                                   max_length=128)\n",
    "        encodings['score'] = torch.tensor(batch_scores)\n",
    "        return encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = {\n",
    "            'text': self.x_train.iloc[i],\n",
    "            'score': self.y_train.iloc[i]\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affecting-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Task1Dataset(tokenizer, full_train_dataset)\n",
    "val_dataset = Task1Dataset(tokenizer, full_val_dataset)\n",
    "\n",
    "print(\"Datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collective-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "\n",
    "class BertRegressor(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return self.linear(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extended-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class BertRegressorTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop('score')\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss_fn = nn.MSELoss().to(device)\n",
    "        loss = loss_fn(outputs.view(-1), labels.to(device=device, dtype=torch.float))\n",
    "\n",
    "        return (loss.sqrt(), outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "permanent-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertRegressor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertRegressor from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertRegressor from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertRegressor were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['linear.1.weight', 'linear.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertRegressor.from_pretrained(PRETRAINED_WEIGHTS)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './experiment/bert_regressor',\n",
    "    overwrite_output_dir = True,\n",
    "    learning_rate = 0.0001,\n",
    "    logging_steps = VAL_STEPS,\n",
    "    evaluation_strategy = 'steps',\n",
    "    label_names = ['score'],\n",
    "    eval_steps = VAL_STEPS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = NUM_EPOCHS,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_loss'\n",
    ")\n",
    "\n",
    "trainer = BertRegressorTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = train_dataset.collate_fn_padd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tutorial-incentive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3624' max='3624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3624/3624 24:04, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.642403</td>\n",
       "      <td>5.309800</td>\n",
       "      <td>455.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.687756</td>\n",
       "      <td>5.334800</td>\n",
       "      <td>453.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.632130</td>\n",
       "      <td>5.331200</td>\n",
       "      <td>453.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.683219</td>\n",
       "      <td>5.341200</td>\n",
       "      <td>452.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.628278</td>\n",
       "      <td>5.339900</td>\n",
       "      <td>453.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.576745</td>\n",
       "      <td>5.348200</td>\n",
       "      <td>452.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.610686</td>\n",
       "      <td>5.352700</td>\n",
       "      <td>451.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.622573</td>\n",
       "      <td>5.336900</td>\n",
       "      <td>453.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.629465</td>\n",
       "      <td>5.343200</td>\n",
       "      <td>452.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.688966</td>\n",
       "      <td>5.375800</td>\n",
       "      <td>449.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.609686</td>\n",
       "      <td>5.349600</td>\n",
       "      <td>452.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.600113</td>\n",
       "      <td>5.376400</td>\n",
       "      <td>449.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.624052</td>\n",
       "      <td>5.352900</td>\n",
       "      <td>451.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.643242</td>\n",
       "      <td>5.349200</td>\n",
       "      <td>452.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.629246</td>\n",
       "      <td>5.348400</td>\n",
       "      <td>452.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.651679</td>\n",
       "      <td>5.353300</td>\n",
       "      <td>451.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.622869</td>\n",
       "      <td>5.353400</td>\n",
       "      <td>451.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.656708</td>\n",
       "      <td>5.360000</td>\n",
       "      <td>451.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.613124</td>\n",
       "      <td>5.362300</td>\n",
       "      <td>451.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.626327</td>\n",
       "      <td>5.355000</td>\n",
       "      <td>451.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.654442</td>\n",
       "      <td>5.352200</td>\n",
       "      <td>451.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>5.366200</td>\n",
       "      <td>450.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.631637</td>\n",
       "      <td>5.358400</td>\n",
       "      <td>451.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.636626</td>\n",
       "      <td>5.366500</td>\n",
       "      <td>450.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.640158</td>\n",
       "      <td>5.355300</td>\n",
       "      <td>451.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.624734</td>\n",
       "      <td>5.354900</td>\n",
       "      <td>451.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.632539</td>\n",
       "      <td>5.361400</td>\n",
       "      <td>451.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.640066</td>\n",
       "      <td>5.368600</td>\n",
       "      <td>450.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.633764</td>\n",
       "      <td>5.362800</td>\n",
       "      <td>451.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.628472</td>\n",
       "      <td>5.383500</td>\n",
       "      <td>449.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.616432</td>\n",
       "      <td>5.370700</td>\n",
       "      <td>450.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.628336</td>\n",
       "      <td>5.369800</td>\n",
       "      <td>450.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.618125</td>\n",
       "      <td>5.592200</td>\n",
       "      <td>432.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.625337</td>\n",
       "      <td>5.425900</td>\n",
       "      <td>445.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.625256</td>\n",
       "      <td>5.361000</td>\n",
       "      <td>451.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.622918</td>\n",
       "      <td>5.358300</td>\n",
       "      <td>451.453000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3624, training_loss=0.14549282224433574, metrics={'train_runtime': 1444.6403, 'train_samples_per_second': 2.509, 'total_flos': 2302834835745792, 'epoch': 6.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "united-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blessed-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convenient-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(target, output):\n",
    "    sq_error = (output - target)**2\n",
    "    mse = torch.mean(sq_error)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse.item()\n",
    "\n",
    "\n",
    "def predict_funniness(text, tokenizer, model): \n",
    "    model.eval()\n",
    "    encodings = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    return model(**encodings)\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, dataset, testing=False):\n",
    "    with torch.no_grad():\n",
    "        text = dataset['text']\n",
    "        score = dataset['score'].tolist()\n",
    "        score = torch.Tensor(score)\n",
    "\n",
    "        pred = predict_funniness(text.tolist(), tokenizer, model)\n",
    "        mse, rmse = compute_metrics(score, pred.view(-1))\n",
    "        print(f'Eval on {\"test set\"if testing else \"val set\"} MSE:{mse} RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "twenty-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval on test set MSE:0.3338930606842041 RMSE: 0.5778347849845886\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, tokenizer, full_test_dataset, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "talented-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text = full_val_dataset['text']\n",
    "    score = full_val_dataset['score'].tolist()\n",
    "    score = torch.Tensor(score)\n",
    "\n",
    "    pred = predict_funniness(text.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nominated-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8213)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
